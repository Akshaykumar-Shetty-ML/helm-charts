global:
  secrets:
    gcs:
      enable: false
      # Set all of the below entities when Google-Service-Account is being used. Recommended approach unless K8s is running outside GCP (K8s outside GCP flow which will use JSON service-account-keys is not yet implemented)
      use_google_service_account: true
      GCP_SERVICE_ACCOUNT_EMAIL: "XXXX-compute@developer.gserviceaccount.com OR XXXX@XXXX.iam.gserviceaccount.com"
      GCP_DEFAULT_REGION: us-west1
      GCP_DEFAULT_ZONE: us-west1-c
    aws:
      enable: true
      # Set use_iam_role = true and AWS_IAM_ROLE_ARN to appropriate IAM ARN in case of IAM role usage
      use_iam_role: false
      AWS_IAM_ROLE_ARN: arn:aws:iam::XXXX:role/XXXX
      # Set all of the below entities when IAM role isn't being used. This is not recommended unless K8s is running outside AWS
      AWS_ACCESS_KEY_ID: XXXX
      AWS_SECRET_ACCESS_KEY: XXXX
      AWS_DEFAULT_REGION: us-west-2

replicaCount: 1
nameOverride: ""
fullnameOverride: ""

schedulerName: sf-scheduler

rbac:
  create: true

serviceAccount:
  create: true
  name:

image:
  repository: lightbend/spark-history-server
  tag: 2.4.0
  pullPolicy: IfNotPresent

service:
  type: NodePort
  nodePort: 32080
  port:
    number: 18080
    name: http-historyport
  annotations: {}

environment:
# Note: do not configure Spark history events directory using SPARK_HISTORY_OPTS. It will be
# configured by this chart based on the values in "pvc", "gcs" or "hdfs" attribute.
  # SPARK_HISTORY_OPTS: ...
  # SPARK_DAEMON_MEMORY: 1g
  # SPARK_DAEMON_JAVA_OPTS: ...
  # SPARK_DAEMON_CLASSPATH: ...
  # SPARK_PUBLIC_DNS: ...

podAnnotations: {}

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  #
  # To let the application start up quickly give it a big limit
  # limits:
  #  cpu: 1000m
  #  memory: 1Gi
  # requests:
  #  cpu: 100m
  #  memory: 512Mi

ingress:
  enabled: false
  annotations: {}
  # kubernetes.io/ingress.class: nginx
  # kubernetes.io/tls-acme: "true"
  path: /
  hosts:
    - spark-history-server.example.com
  tls: []
  #  - secretName:spark-history-server.example.com
  #    hosts:
  #      - spark-history-server.example.com

pvc:
  # to use a file system path for Spark events dir, set 'enablePVC' to true and mention the
  # name of an already created persistent volume claim in existingClaimName.
  # The volume will be mounted on /data in the pod
  enablePVC: false
  existingClaimName: nfs-pvc
  eventsDir: "/"

# Settings for the sub-chart
# When pvc.enablePVC is true, make sure:
# pvc.existingClaimName == nfs.pvcName
nfs:
  enableExampleNFS: false
  pvName: nfs-pv
  pvcName: nfs-pvc

gcs:
  secret: history-secrets
  key: sparkonk8s.json
  logDirectory: gs://spark-hs/

hdfs:
  enableHDFS: false
  hdfsSiteConfigMap: hdfs-site
  coreSiteConfigMap: core-site
  logDirectory: hdfs://hdfs/history/
  HADOOP_CONF_DIR: /etc/hadoop

s3:
  logDirectory: s3a://sparkhs/spark-hs
  # custom s3 endpoint. Keep default for using aws s3 endpoint
  endpoint: default

wasbs:
  enableWASBS: false
  sasKeyMode: true
  secret: azure-secrets
  sasKeyName: azure-blob-sas-key
  storageAccountKeyName: azure-storage-account-key
  storageAccountNameKeyName: azure-storage-account-name
  containerKeyName: azure-blob-container-name
  logDirectory: wasbs:///spark-hs

imagePullSecrets: []

nodeSelector: {}

tolerations: []

affinity: {}
