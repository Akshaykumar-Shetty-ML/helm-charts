---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: release-name-prometheus-adapter-new
  namespace: default
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: release-name-nginx-ingress
  namespace: default
automountServiceAccountToken: true
---
# Source: sf-datapath/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: release-name-usercreds
  namespace: default
type: Opaque
stringData:
  auth: control:$apr1$pjbvxsh8$UnVE6PZ/3874l.cJ.Xc1e0
---
# Source: sf-datapath/charts/archival-kafka-connect/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-archival-kafka-connect-kafka-log
  labels:
    app: archival-kafka-connect
    chart: archival-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
data:
  connect-log4j.properties: |+
    log4j.rootLogger=INFO, console
    log4j.logger.org.maplelabs.smt=INFO, console
    log4j.additivity.org.maplelabs.smt=false
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.Target=System.out
    log4j.appender.console.immediateFlush=true
    log4j.appender.console.encoding=UTF-8
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.conversionPattern=[%d] %p %m (%c)%n
    log4j.appender.console.Threshold=DEBUG
    log4j.logger.org.apache.zookeeper=ERROR
    log4j.logger.org.I0Itec.zkclient=ERROR
    log4j.logger.org.reflections=ERROR
    log4j.logger.org.eclipse.jetty=ERROR
    log4j.logger.kafka=ERROR
    log4j.logger.org.apache.kafka.clients.admin.AdminClientConfig=ERROR
---
# Source: sf-datapath/charts/archival-kafka-connect/templates/jmx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-archival-kafka-connect-jmx-configmap
  labels:
    app: archival-kafka-connect
    chart: archival-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
data:
  jmx-kafka-connect-prometheus.yml: |+
    startDelaySeconds: 90
    jmxUrl: service:jmx:rmi:///jndi/rmi://localhost:5555/jmxrmi
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    ssl: false
    whitelistObjectNames:
    - "java.lang:*"
    - "java.nio:*"
    - "kafka.connect:type=connect-worker-metrics,*"
    - "kafka.connect:type=connect-worker-rebalance-metrics,*"
    - "kafka.connect:type=task-error-metrics,*"
    - "kafka.connect:type=connector-task-metrics,*"
    - "kafka.connect:type=source-task-metrics,*"
    - "kafka.connect:type=sink-task-metrics,*"
    - "kafka.connect:type=connect-metrics,*"
    - "kafka.connect:type=connect-node-metrics,*"
    - "kafka.connect:type=connect-coordinator-metrics,*"
    - "kafka.producer:type=producer-metrics,*"
    - "kafka.producer:type=producer-node-metrics,*"
    - "kafka.producer:type=producer-topic-metrics,*"
    - "kafka.consumer:type=consumer-metrics,*"
    - "kafka.consumer:type=consumer-fetch-manager-metrics,*"
    - "kafka.consumer:type=consumer-node-metrics,*"
    - "kafka.consumer:type=consumer-coordinator-metrics,*"
    rules:
    - pattern: "kafka.producer<type=producer-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.producer<type=producer-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: "kafka.producer<type=producer-topic-metrics, client-id=(.+), topic=(.+)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_topic_$3
      labels:
        clientID: "$1"
        topic: "$2"
        clientID_topic: "$1-$2"
    - pattern: "kafka.consumer<type=consumer-fetch-manager-metrics, client-id=(.+), topic=(.+), partition=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_topic_partn_$4
      labels:
        clientID: "$1"
        topic: "$2"
        partition: "$3"
        clientID_topic_partition: "$1-$2-$3"
    - pattern: "kafka.consumer<type=consumer-fetch-manager-metrics, client-id=(.+), topic=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_topic_$3
      labels:
        clientID: "$1"
        topic: "$2"
        clientID_topic: "$1-$2"
    - pattern: "kafka.consumer<type=consumer-fetch-manager-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.consumer<type=consumer-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.consumer<type=consumer-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: "kafka.consumer<type=consumer-coordinator-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_coord_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.connect<type=connect-worker-metrics, connector=(.+)><>(.+)\\:"
      cache: true
      name: kafka_connect_worker_connector_$2
      labels:
        connector: "$1"
    - pattern: "kafka.connect<type=connect-worker-metrics><>(.+)\\:"
      cache: true
      name: kafka_connect_worker_$1
    - pattern: "kafka.connect<type=connect-worker-rebalance-metrics><>(.+)\\:"
      cache: true
      name: kafka_connect_worker_rebalance_$1
    - pattern: "kafka.connect<type=task-error-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_task_error_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=connector-task-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_task_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=sink-task-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_sink_task_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=source-task-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_source_task_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=connect-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_connect_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.connect<type=connect-coordinator-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_connect_client_coord_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.connect<type=connect-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_connect_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: java.lang<type=(.+), name=(.+)><(.+)>(\w+)
      name: java_lang_$1_$4_$3_$2
      cache: true
    - pattern: java.lang<name=(.+), type=(.+)><(.+)>(\w+)
      name: java_lang_$2_$4_$3_$1
      cache: true
    - pattern: java.lang<type=(.+), name=(.+)><>(\w+)
      name: java_lang_$1_$3_$2
      cache: true
    - pattern: java.lang<name=(.+), type=(.+)><>(\w+)
      name: java_lang_$2_$3_$1
      cache: true
    - pattern: java.lang<type=(.*)>
      cache: true
    - pattern: java.nio<type=(.+), name=(.+)><>(\w+)
      name: java_nio_$1_$3_$2
      cache: true
---
# Source: sf-datapath/charts/authenticator/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: release-name-authenticator
  namespace: default
data:
  config.json: |-
    {
      "admin": {
        "profileKey": "admin",
        "userName": "admin",
        "password": "admin"
      },
      "db": {
        "name":  "archival",
        "user":  "archive",
        "password": "archive123",
        "host": "release-name-postgresql",
        "port": 5432
      },
      "signingKey": "mysecretkey",
      "serverPort": ":5006"
    }
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-prometheus-adapter-new
  labels:
    app: prometheus-adapter-new
    chart: prometheus-adapter-3.0.1
    release: release-name
    heritage: Helm
data:
  config.yaml: |
    rules:
    - metricsQuery: avg_over_time(<<.Series>>{kubernetes_pod_name=~".*es-kafka-connect.*"}[3m])
      name:
        as: es_connect_heap_utilization_pcnt
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="heap_utilization_pcnt"}'
    - metricsQuery: (avg_over_time(<<.Series>>{kubernetes_pod_name=~".*es-kafka-connect.*"}[3m])
        * 100.0) / (200 * 1048576)
      name:
        as: es_connect_nio_utilization_pcnt
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="nio_memory_utilization"}'
    - metricsQuery: avg_over_time(<<.Series>>{kubernetes_pod_name=~".*archival-kafka-connect.*"}[3m])
      name:
        as: archival_connect_heap_utilization_pcnt
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="heap_utilization_pcnt"}'
    - metricsQuery: (avg_over_time(<<.Series>>{kubernetes_pod_name=~".*archival-kafka-connect.*"}[3m])
        * 100.0) / (100 * 1048576)
      name:
        as: archival_connect_nio_utilization_pcnt
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="nio_memory_utilization"}'
    - metricsQuery: avg_over_time(<<.Series>>{kubernetes_pod_name=~".*kafka-rest.*"}[3m])
      name:
        as: kafka_rest_heap_utilization_pcnt
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="heap_utilization_pcnt"}'
    - metricsQuery: <<.Series>>{kubernetes_pod_name=~".*python3-sftrace.*"}
      name:
        as: sftrace_queued_events
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="sftrace_queued_events"}'
    - metricsQuery: <<.Series>>{kubernetes_pod_name=~".*sf-presto-worker.*"}
      name:
        as: presto_running_drivers
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="presto_running_drivers"}'
    - metricsQuery: <<.Series>>{kubernetes_pod_name=~".*sf-presto-worker.*"}
      name:
        as: presto_queued_queries
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="presto_queued_queries"}'
    - metricsQuery: <<.Series>>{kubernetes_pod_name=~".*vizbuilder-celery-alert.*"}
      name:
        as: num_executing_tasks
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
          kubernetes_pod_name:
            resource: pod
      seriesQuery: '{__name__="num_executing_tasks"}'
    externalRules:
    - metricsQuery: avg_over_time(<<.Series>>{kubernetes_namespace="kafka", release="kafka"}[3m])
      name:
        as: kafka_bytesinpersec_oneminuterate
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
      seriesQuery: '{__name__="kafka_incoming_byte_rate"}'
    - metricsQuery: avg_over_time(<<.Series>>{kubernetes_namespace="kafka", release="kafka"}[3m])
      name:
        as: kafka_bytesinpersec_oneminuterate_for_log_and_metric
        matches: ""
      resources:
        overrides:
          kubernetes_namespace:
            resource: namespace
      seriesQuery: '{__name__="kafka_incoming_byte_rate_for_log_and_metric"}'
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus/templates/server/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server-new"
    app: prometheus
    release: release-name
    chart: prometheus-15.0.2
    heritage: Helm
  name: release-name-prometheus-server-new
  namespace: default
data:
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 5s
      query_log_file: /prometheus/query.log
      scrape_interval: 5s
      scrape_timeout: 3s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
  recording_rules.yml: |
    groups:
    - name: kafka_elements
      rules:
      - expr: (java_lang_memory_heapmemoryusage_used * 100.0) / java_lang_memory_heapmemoryusage_max
        record: heap_utilization_pcnt
      - expr: java_nio_bufferpool_memoryused_direct + java_nio_bufferpool_memoryused_mapped
        record: nio_memory_utilization
      - expr: sum by (app, kubernetes_namespace, release)(kafka_server_brokertopicmetrics_total_bytesinpersec_oneminuterate)
        record: kafka_incoming_byte_rate
      - expr: sum by (app, kubernetes_namespace, release)(kafka_server_brokertopicmetrics_bytesinpersec_oneminuterate{topic=~"^(log|metric).*"})
        record: kafka_incoming_byte_rate_for_log_and_metric
  rules: |
    {}
  web.config.yml: |
    basic_auth_users:
      user: $2b$12$DNHTLFJ.sTLAmgGyBx3n..fOEbvDdp5lWTl3ORP3m9tYdo5Nq0uJG
---
# Source: sf-datapath/charts/cp-kafka-rest/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-cp-kafka-rest-kafka-log
  labels:
    app: cp-kafka-rest
    chart: cp-kafka-rest-0.1.0
    release: release-name
    heritage: Helm
data:
  connect-log4j.properties: |+
    log4j.rootLogger=INFO, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.Target=System.out
    log4j.appender.console.immediateFlush=true
    log4j.appender.console.encoding=UTF-8
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.conversionPattern=[%d] %p %m (%c)%n
    log4j.appender.console.Threshold=DEBUG
    log4j.logger.org.apache.zookeeper=ERROR
    log4j.logger.org.I0Itec.zkclient=ERROR
    log4j.logger.org.reflections=ERROR
    log4j.logger.io.confluent.rest-utils.requests=WARN
---
# Source: sf-datapath/charts/cp-kafka-rest/templates/jmx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-cp-kafka-rest-jmx-configmap
  labels:
    app: cp-kafka-rest
    chart: cp-kafka-rest-0.1.0
    release: release-name
    heritage: Helm
data:
  jmx-kafka-rest-prometheus.yml: |+
    startDelaySeconds: 120
    jmxUrl: service:jmx:rmi:///jndi/rmi://localhost:5555/jmxrmi
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    ssl: false
    whitelistObjectNames:
    - "java.lang:*"
    - "java.nio:*"
    - "org.eclipse.jetty.util.thread:*"
    - "kafka.rest:type=jersey-metrics,*"
    - "kafka.rest:type=jetty-metrics,*"
    - "kafka.rest:type=producer-metrics,*"
    - "kafka.rest:type=producer-node-metrics,*"
    - "kafka.rest:type=producer-topic-metrics,*"
    - "kafka.rest:type=consumer-metrics,*"
    - "kafka.rest:type=consumer-fetch-manager-metrics,*"
    - "kafka.rest:type=consumer-node-metrics,*"
    - "kafka.rest:type=consumer-coordinator-metrics,*"
    rules:
    - pattern: "kafka.rest<type=producer-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.rest<type=producer-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: "kafka.rest<type=producer-topic-metrics, client-id=(.+), topic=(.+)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_topic_$3
      labels:
        clientID: "$1"
        topic: "$2"
        clientID_topic: "$1-$2"
    - pattern: "kafka.rest<type=consumer-fetch-manager-metrics, client-id=(.+), topic=(.+), partition=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_topic_partn_$4
      labels:
        clientID: "$1"
        topic: "$2"
        partition: "$3"
        clientID_topic_partition: "$1-$2-$3"
    - pattern: "kafka.rest<type=consumer-fetch-manager-metrics, client-id=(.+), topic=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_topic_$3
      labels:
        clientID: "$1"
        topic: "$2"
        clientID_topic: "$1-$2"
    - pattern: "kafka.rest<type=consumer-fetch-manager-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.rest<type=consumer-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.rest<type=consumer-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: "kafka.rest<type=consumer-coordinator-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_coord_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.rest<type=jersey-metrics><>(.+?)\\.(.+)\\+v2\\.([^\\.]+)\\:"
      cache: true
      name: kafka_rest_api_$3
      labels:
        apiVersion: "v2"
        apiGroup: "$1"
        api: "$2"
        apiGroup_api: "$1-$2"
    - pattern: "kafka.rest<type=jersey-metrics><>v3\\.([^\\.]+)\\.(.+?)\\.([^\\.]+)\\:"
      cache: true
      name: kafka_rest_api_$3
      labels:
        apiVersion: "v3"
        apiGroup: "$1"
        api: "$2"
        apiGroup_api: "$1-$2"
    - pattern: "kafka.rest<type=jersey-metrics, http_status_code=(.+)><>(.+?)\\.(.+)\\+v2\\.([^\\.]+)\\:"
      cache: true
      name: kafka_rest_api_$4
      labels:
        apiVersion: "v2"
        apiGroup: "$2"
        api: "$3"
        status-code: "$1"
        apiGroup_api: "$2-$3"
    - pattern: "kafka.rest<type=jersey-metrics, http_status_code=(.+)><>v3\\.([^\\.]+)\\.(.+?)\\.([^\\.]+)\\:"
      cache: true
      name: kafka_rest_api_$4
      labels:
        apiVersion: "v3"
        apiGroup: "$2"
        api: "$3"
        status-code: "$1"
        apiGroup_api: "$2-$3"
    - pattern: kafka.rest<type=jersey-metrics>
      cache: true
    - pattern: kafka.rest<type=jetty-metrics>
      cache: true
    - pattern: org.eclipse.jetty.util.thread<type=(.+), id=(.+)><>([^:]+)
      cache: true
      name: kafka_rest_jetty_$1_$3
      labels:
        id: "$2"
    - pattern: java.lang<type=(.+), name=(.+)><(.+)>(\w+)
      name: java_lang_$1_$4_$3_$2
      cache: true
    - pattern: java.lang<name=(.+), type=(.+)><(.+)>(\w+)
      name: java_lang_$2_$4_$3_$1
      cache: true
    - pattern: java.lang<type=(.+), name=(.+)><>(\w+)
      name: java_lang_$1_$3_$2
      cache: true
    - pattern: java.lang<name=(.+), type=(.+)><>(\w+)
      name: java_lang_$2_$3_$1
      cache: true
    - pattern: java.lang<type=(.*)>
      cache: true
    - pattern: java.nio<type=(.+), name=(.+)><>(\w+)
      name: java_nio_$1_$3_$2
      cache: true
---
# Source: sf-datapath/charts/cp-schema-registry/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-cp-schema-registry-kafka-log
  labels:
    app: cp-schema-registry
    chart: cp-schema-registry-0.1.0
    release: release-name
    heritage: Helm
data:
  connect-log4j.properties: |+
    log4j.rootLogger=INFO, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.Target=System.out
    log4j.appender.console.immediateFlush=true
    log4j.appender.console.encoding=UTF-8
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.conversionPattern=[%d] %p %m (%c)%n
    log4j.appender.console.Threshold=DEBUG
    log4j.logger.org.apache.zookeeper=ERROR
    log4j.logger.org.I0Itec.zkclient=ERROR
    log4j.logger.org.reflections=ERROR
---
# Source: sf-datapath/charts/cp-schema-registry/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-cp-schema-registry-schema-generator-configmap
  labels:
    app: cp-schema-registry
    chart: cp-schema-registry-0.1.0
    release: release-name
    heritage: Helm
data:
  config.yaml: |+
    intervalInMin: 60
    db:
      name:  "archival"
      user:  "archive"
      password: "archive123"
      host: 
      port: 5432
    targets:
    - enabled: true
      name: schema-registry
      url: "http://archival-ingest-controller/ingest/schema-registry"
---
# Source: sf-datapath/charts/es-kafka-connect/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-es-kafka-connect-kafka-log
  labels:
    app: es-kafka-connect
    chart: es-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
data:
  connect-log4j.properties: |+
    log4j.rootLogger=INFO, console
    log4j.logger.org.maplelabs.smt=INFO, console
    log4j.additivity.org.maplelabs.smt=false
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.Target=System.out
    log4j.appender.console.immediateFlush=true
    log4j.appender.console.encoding=UTF-8
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.conversionPattern=[%d] %p %m (%c)%n
    log4j.appender.console.Threshold=DEBUG
    log4j.logger.org.apache.zookeeper=ERROR
    log4j.logger.org.I0Itec.zkclient=ERROR
    log4j.logger.org.reflections=ERROR
    log4j.logger.org.eclipse.jetty=ERROR
    log4j.logger.kafka=ERROR
    log4j.logger.org.apache.kafka.clients.admin.AdminClientConfig=ERROR
---
# Source: sf-datapath/charts/es-kafka-connect/templates/jmx-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-es-kafka-connect-jmx-configmap
  labels:
    app: es-kafka-connect
    chart: es-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
data:
  jmx-kafka-connect-prometheus.yml: |+
    startDelaySeconds: 90
    jmxUrl: service:jmx:rmi:///jndi/rmi://localhost:5555/jmxrmi
    lowercaseOutputName: true
    lowercaseOutputLabelNames: true
    ssl: false
    whitelistObjectNames:
    - "java.lang:*"
    - "java.nio:*"
    - "kafka.connect:type=connect-worker-metrics,*"
    - "kafka.connect:type=connect-worker-rebalance-metrics,*"
    - "kafka.connect:type=task-error-metrics,*"
    - "kafka.connect:type=connector-task-metrics,*"
    - "kafka.connect:type=source-task-metrics,*"
    - "kafka.connect:type=sink-task-metrics,*"
    - "kafka.connect:type=connect-metrics,*"
    - "kafka.connect:type=connect-node-metrics,*"
    - "kafka.connect:type=connect-coordinator-metrics,*"
    - "kafka.producer:type=producer-metrics,*"
    - "kafka.producer:type=producer-node-metrics,*"
    - "kafka.producer:type=producer-topic-metrics,*"
    - "kafka.consumer:type=consumer-metrics,*"
    - "kafka.consumer:type=consumer-fetch-manager-metrics,*"
    - "kafka.consumer:type=consumer-node-metrics,*"
    - "kafka.consumer:type=consumer-coordinator-metrics,*"
    rules:
    - pattern: "kafka.producer<type=producer-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.producer<type=producer-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: "kafka.producer<type=producer-topic-metrics, client-id=(.+), topic=(.+)><>(.+)\\:"
      cache: true
      name: kafka_producer_client_topic_$3
      labels:
        clientID: "$1"
        topic: "$2"
        clientID_topic: "$1-$2"
    - pattern: "kafka.consumer<type=consumer-fetch-manager-metrics, client-id=(.+), topic=(.+), partition=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_topic_partn_$4
      labels:
        clientID: "$1"
        topic: "$2"
        partition: "$3"
        clientID_topic_partition: "$1-$2-$3"
    - pattern: "kafka.consumer<type=consumer-fetch-manager-metrics, client-id=(.+), topic=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_topic_$3
      labels:
        clientID: "$1"
        topic: "$2"
        clientID_topic: "$1-$2"
    - pattern: "kafka.consumer<type=consumer-fetch-manager-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.consumer<type=consumer-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.consumer<type=consumer-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: "kafka.consumer<type=consumer-coordinator-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_consumer_client_coord_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.connect<type=connect-worker-metrics, connector=(.+)><>(.+)\\:"
      cache: true
      name: kafka_connect_worker_connector_$2
      labels:
        connector: "$1"
    - pattern: "kafka.connect<type=connect-worker-metrics><>(.+)\\:"
      cache: true
      name: kafka_connect_worker_$1
    - pattern: "kafka.connect<type=connect-worker-rebalance-metrics><>(.+)\\:"
      cache: true
      name: kafka_connect_worker_rebalance_$1
    - pattern: "kafka.connect<type=task-error-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_task_error_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=connector-task-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_task_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=sink-task-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_sink_task_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=source-task-metrics, connector=(.+), task=(\\d)+><>(.+)\\:"
      cache: true
      name: kafka_connect_source_task_$3
      labels:
        connector: "$1"
        task: "$2"
        connector_task: "$1_$2"
    - pattern: "kafka.connect<type=connect-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_connect_client_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.connect<type=connect-coordinator-metrics, client-id=(.+)><>(.+)\\:"
      cache: true
      name: kafka_connect_client_coord_$2
      labels:
        clientID: "$1"
    - pattern: "kafka.connect<type=connect-node-metrics, client-id=(.+), node-id=node-(\\d)><>(.+)\\:"
      cache: true
      name: kafka_connect_client_node_$3
      labels:
        clientID: "$1"
        nodeID: "$2"
        clientID_nodeID: "$1-$2"
    - pattern: java.lang<type=(.+), name=(.+)><(.+)>(\w+)
      name: java_lang_$1_$4_$3_$2
      cache: true
    - pattern: java.lang<name=(.+), type=(.+)><(.+)>(\w+)
      name: java_lang_$2_$4_$3_$1
      cache: true
    - pattern: java.lang<type=(.+), name=(.+)><>(\w+)
      name: java_lang_$1_$3_$2
      cache: true
    - pattern: java.lang<name=(.+), type=(.+)><>(\w+)
      name: java_lang_$2_$3_$1
      cache: true
    - pattern: java.lang<type=(.*)>
      cache: true
    - pattern: java.nio<type=(.+), name=(.+)><>(\w+)
      name: java_nio_$1_$3_$2
      cache: true
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: release-name-nginx-ingress-controller
  namespace: default
data:
  allow-snippet-annotations: "true"
  access-log-path: "/var/log/nginx/access-custom.log"
  error-log-path: "/var/log/nginx/error-custom.log"
  log-format-upstream: "$http_x_forwarded_for $remote_user [$time_local] \"$request\" $status $body_bytes_sent \"$http_referer\" \"$http_user_agent\" \"$http_referer\" rt=$request_time uct=$upstream_connect_time uht=$upstream_header_time urt=$upstream_response_time rs=$request_length"
---
# Source: sf-datapath/charts/sfk-interface/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
    name: release-name-sfk-interface-config
    namespace: default
data:
    config.json: |-
        {
            "kafka": {
                "brokers": "localhost:9092,localhost1:9092",
                "rest_proxy": "http://release-name-cp-kafka-rest:8082",
                "rest_auth": "http://release-name-authenticator",
                "connect": "http://release-name-es-kafka-connect:8083"
            },
            "agent_ep": "https://127.0.0.1:443",
            "signature_and_kafka_apis": "http://release-name-signatures-and-kafka-apis",
            "cluster_byte_rate_quota": 1157400,
            "prometheus_endpoint": "http://release-name-prometheus-server-new",
            "prometheus_auth": "snappyflow",
            "sys_rsvd_cluster_byte_rate_quota_pcnt": 0,
            "db": {
                "name":  "archival",
                "user":  "archive",
                "password": "archive123",
                "host": "release-name-postgresql",
                "port": 5432
            },
            "max_tasks_per_topic": 3,
            "topic_type_details": [{"num_partitions":3,"replication_factor":2,"retention_ms":"86400000","type":"log"},{"num_partitions":3,"replication_factor":2,"retention_ms":"86400000","type":"metric"},{"num_partitions":3,"replication_factor":2,"retention_ms":"3600000","type":"control"},{"num_partitions":3,"replication_factor":2,"retention_ms":"3600000","type":"trace"},{"num_partitions":3,"replication_factor":2,"retention_ms":"3600000","type":"profile"}],
            "quotas_enabled": true
        }
    es_sink_defaults.json: |-
        {
            "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
            "auto.create.indices.at.start": "false",
            "errors.tolerance": "all",
            "drop.invalid.message": "true",
            "behavior.on.malformed.documents": "warn",
            "behavior.on.null.values": "delete",
            "max.buffered.records": "5000",
            "retry.backoff.ms": "3000",
            "max.retries": "3",
            "connection.timeout.ms": "5000",
            "read.timeout.ms": "30000",
            "flush.timeout.ms": "150000",
            "batch.size": "5000",
            "max.in.flight.requests": "1",
            "linger.ms": "1000",
            "schema.ignore": "true",
            "value.converter.schemas.enable": "false",
            "value.converter": "org.apache.kafka.connect.json.JsonConverter",
            "transforms": "filter,router,insertsignature",
            "transforms.throttle.type": "apm.kafka.connect.transforms.Throttle",
            "transforms.filter.type": "apm.kafka.connect.transforms.APMFilter",
            "transforms.router.type": "apm.kafka.connect.transforms.APMRouter",
            "transforms.insertsignature.type": "com.signature.InsertSignature$Value",
            "transforms.insertsignature.connectorname.field": "elastic-search"
        }
---
# Source: sf-datapath/charts/signatures-and-kafka-apis/templates/configmap.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: release-name-signatures-and-kafka-apis
  namespace: default
data:
  config.json: |-
    {
      "db": {
        "name":  "archival",
        "user":  "archive",
        "password": "archive123",
        "host": "release-name-postgresql",
        "port": 5432
      },
      "default-patterns": "upstream timed out,upstream server temporarily disabled while reading response header from upstream,upstream server temporarily disabled while connecting to upstream,upstream prematurely closed connection while reading response header from upstream,session opened for user,session closed for user,possible break,invalid user,dhcprequest,dhcpack",
      "set-default-patterns": false,
      "registered-urls": ["http://archival-ingest-controller/ingest/datasets/updatepatterns","http://release-name-sfk-interface/signpatterns"
      ]
    }
---
# Source: sf-datapath/templates/system-migration.yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: release-name-system-migration-config
  namespace: default
data:
  config.json: |-
    {
      "db": {
        "name": "archival",
        "user": "archive",
        "password": "archive123",
        "host": "",
        "port": 5432
      },
      "kafka_brokers": "localhost:9092,localhost1:9092",
      "es_kafka_connect_url": "http://release-name-es-kafka-connect:8083",
      "archival_kafka_connect_url": "http://release-name-archival-kafka-connect:8083",
      "signature_and_kafka_apis": "http://release-name-signatures-and-kafka-apis",
      "kafka_rest_auth_url": "http://release-name-authenticator",
      "cluster_byte_rate_quota": 1157400,
      "sys_rsvd_cluster_byte_rate_quota_pcnt": 0,
      "max_tasks_per_topic": 3,
      "topic_type_details": [{"num_partitions":3,"replication_factor":2,"retention_ms":"86400000","type":"log"},{"num_partitions":3,"replication_factor":2,"retention_ms":"86400000","type":"metric"},{"num_partitions":3,"replication_factor":2,"retention_ms":"3600000","type":"control"},{"num_partitions":3,"replication_factor":2,"retention_ms":"3600000","type":"trace"},{"num_partitions":3,"replication_factor":2,"retention_ms":"3600000","type":"profile"}],
      "quotas_enabled": true
    }
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus/templates/server/pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    component: "server-new"
    app: prometheus
    release: release-name
    chart: prometheus-15.0.2
    heritage: Helm
  name: release-name-prometheus-server-new
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "50Gi"
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/cluster-role-resource-reader.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-resource-reader
rules:
- apiGroups:
  - ""
  resources:
  - namespaces
  - pods
  - services
  - configmaps
  verbs:
  - get
  - list
  - watch
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/custom-metrics-cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-server-resources
rules:
- apiGroups:
  - custom.metrics.k8s.io
  resources: ["*"]
  verbs: ["*"]
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/external-metrics-cluster-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-external-metrics
rules:
- apiGroups:
  - "external.metrics.k8s.io"
  resources:
  - "*"
  verbs:
  - list
  - get
  - watch
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus/templates/server/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    component: "server-new"
    app: prometheus
    release: release-name
    chart: prometheus-15.0.2
    heritage: Helm
  name: release-name-prometheus-server-new
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: sf-datapath/charts/nginx-ingress/templates/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
  name: release-name-nginx-ingress
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
      - endpoints
      - nodes
      - pods
      - secrets
      - namespaces
    verbs:
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - nodes
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
      - get
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/cluster-role-binding-auth-delegator.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-system-auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: release-name-prometheus-adapter-new
  namespace: "default"
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/cluster-role-binding-resource-reader.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-resource-reader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-adapter-new-resource-reader
subjects:
- kind: ServiceAccount
  name: release-name-prometheus-adapter-new
  namespace: "default"
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/custom-metrics-cluster-role-binding-hpa.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-hpa-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-adapter-new-server-resources
subjects:
- kind: ServiceAccount
  name: release-name-prometheus-adapter-new
  namespace: "default"
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/external-metrics-cluster-role-binding-hpa.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-hpa-controller-external-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus-adapter-new-external-metrics
subjects:
- kind: ServiceAccount
  name: horizontal-pod-autoscaler
  namespace: kube-system
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus/templates/server/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "server-new"
    app: prometheus
    release: release-name
    chart: prometheus-15.0.2
    heritage: Helm
  name: release-name-prometheus-server-new
subjects:
  - kind: ServiceAccount
    name: default
    namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-prometheus-server-new
---
# Source: sf-datapath/charts/nginx-ingress/templates/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
  name: release-name-nginx-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-nginx-ingress
subjects:
  - kind: ServiceAccount
    name: release-name-nginx-ingress
    namespace: "default"
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: release-name-nginx-ingress
  namespace: default
rules:
  - apiGroups:
      - ""
    resources:
      - namespaces
    verbs:
      - get
  - apiGroups:
      - ""
    resources:
      - configmaps
      - pods
      - secrets
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - services
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingresses/status
    verbs:
      - update
  - apiGroups:
      - networking.k8s.io
    resources:
      - ingressclasses
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    resourceNames:
      - release-name-nginx-ingress-leader
    verbs:
      - get
      - update
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
  - apiGroups:
      - discovery.k8s.io
    resources:
      - endpointslices
    verbs:
      - list
      - watch
      - get
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/role-binding-auth-reader.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: prometheus-adapter-new-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: release-name-prometheus-adapter-new
  namespace: "default"
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: release-name-nginx-ingress
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-nginx-ingress
subjects:
  - kind: ServiceAccount
    name: release-name-nginx-ingress
    namespace: "default"
---
# Source: sf-datapath/charts/archival-kafka-connect/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-archival-kafka-connect
  labels:
    app: archival-kafka-connect
    chart: archival-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
spec:
  ports:
    - name: arch-connect
      port: 8083
    - port: 5556
      name: jmx-exporter
  selector:
    app: archival-kafka-connect
    release: release-name
---
# Source: sf-datapath/charts/authenticator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-authenticator
  namespace: default
  labels:
    app.kubernetes.io/name: authenticator
    helm.sh/chart: authenticator-0.1.0
    app.kubernetes.io/instance: release-name
    release: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - port: 80
    targetPort: 5006
    protocol: TCP
  selector:
    app.kubernetes.io/name: authenticator
    app.kubernetes.io/instance: release-name
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: release-name-prometheus-adapter-new
  namespace: default
spec:
  ports:
  - port: 443
    protocol: TCP
    targetPort: https
  selector:    
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
  type: ClusterIP
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus/templates/server/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "server-new"
    app: prometheus
    release: release-name
    chart: prometheus-15.0.2
    heritage: Helm
  name: release-name-prometheus-server-new
  namespace: default
spec:
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: 9090
      nodePort: 30000
  selector:
    component: "server-new"
    app: prometheus
    release: release-name
  sessionAffinity: None
  type: "NodePort"
---
# Source: sf-datapath/charts/cp-kafka-rest/templates/external-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-cp-kafka-rest-external
  labels:
    app: cp-kafka-rest
    chart: cp-kafka-rest-0.1.0
    release: release-name
    heritage: Helm
spec:
  selector:
    app: cp-kafka-rest
    release: release-name
  type: NodePort
  externalTrafficPolicy: Cluster
  
  ports:
    
    - name: rest-proxy
      port: 8082
      nodePort: 32002
---
# Source: sf-datapath/charts/cp-kafka-rest/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-cp-kafka-rest
  labels:
    app: cp-kafka-rest
    chart: cp-kafka-rest-0.1.0
    release: release-name
    heritage: Helm
spec:
  ports:
    - name: rest-proxy
      port: 8082
    - port: 5556
      name: jmx-exporter
  selector:
    app: cp-kafka-rest
    release: release-name
---
# Source: sf-datapath/charts/cp-schema-registry/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-cp-schema-registry
  labels:
    app: cp-schema-registry
    chart: cp-schema-registry-0.1.0
    release: release-name
    heritage: Helm
spec:
  ports:
    - name: schema-registry
      port: 8081
  selector:
    app: cp-schema-registry
    release: release-name
---
# Source: sf-datapath/charts/es-kafka-connect/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-es-kafka-connect
  labels:
    app: es-kafka-connect
    chart: es-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
spec:
  ports:
    - name: es-connect
      port: 8083
    - port: 5556
      name: jmx-exporter
  selector:
    app: es-kafka-connect
    release: release-name
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-service-webhook.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: release-name-nginx-ingress-controller-admission
  namespace: default
spec:
  type: ClusterIP
  ports:
    - name: https-webhook
      port: 443
      targetPort: webhook
      appProtocol: https
  selector:
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: controller
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-service.yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: release-name-nginx-ingress-controller
  namespace: default
spec:
  type: NodePort
  ipFamilyPolicy: SingleStack
  ipFamilies: 
    - IPv4
  ports:
    - name: http
      port: 80
      protocol: TCP
      targetPort: http
      appProtocol: http
      nodePort: 32000
    - name: https
      port: 443
      protocol: TCP
      targetPort: https
      appProtocol: https
      nodePort: 32001
  selector:
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: controller
---
# Source: sf-datapath/charts/sfk-interface/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-sfk-interface
  labels:
    helm.sh/chart: sfk-interface-0.1.0
    release: release-name
    app.kubernetes.io/name: sfk-interface
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  selector:
    app.kubernetes.io/name: sfk-interface
    app.kubernetes.io/instance: release-name
---
# Source: sf-datapath/charts/signatures-and-kafka-apis/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-signatures-and-kafka-apis
  namespace: default
  labels:
    app.kubernetes.io/name: signatures-and-kafka-apis
    helm.sh/chart: signatures-and-kafka-apis-0.1.0
    app.kubernetes.io/instance: release-name
    release: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - port: 80
    targetPort: 8888
    protocol: TCP
  selector:
    app.kubernetes.io/name: signatures-and-kafka-apis
    app.kubernetes.io/instance: release-name
---
# Source: sf-datapath/charts/authenticator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-authenticator
  namespace: default
  labels:
    app.kubernetes.io/name: authenticator
    helm.sh/chart: authenticator-0.1.0
    app.kubernetes.io/instance: release-name
    release: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: authenticator
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        checksum/config: 006c9dec7f15f8dd237d524e8de7f84407665540b4dac4841e4f89b58e8b834b
      labels:
        app.kubernetes.io/name: authenticator
        app.kubernetes.io/instance: release-name
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
    spec:
      priorityClassName: release-name-priority-class
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'release-name-postgresql':5432/'archival'; do sleep 3; done"
      containers:
        - name: authenticator
          image: "snappyflowml/authenticator:7"
          imagePullPolicy: IfNotPresent
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
            requests:
              cpu: 10m
              memory: 50Mi
      volumes:
        - name: config-volume
          configMap:
            name: release-name-authenticator
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus-adapter-new
    chart: prometheus-adapter-3.0.1
    release: release-name
    heritage: Helm
  name: release-name-prometheus-adapter-new
  namespace: default
spec:
  replicas: 1
  strategy: 
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  selector:
    matchLabels:      
      app.kubernetes.io/name: prometheus-adapter-new
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      labels:        
        helm.sh/chart: prometheus-adapter-3.0.1
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: prometheus-adapter-new
        app.kubernetes.io/name: prometheus-adapter-new
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "v0.9.1"
      name: prometheus-adapter-new
      annotations:
        checksum/config: 0a99d85fef1b9e190e5545e3cd3c6dc181291dae90218c5eb4ce8c75da079d3f
    spec:
      serviceAccountName: release-name-prometheus-adapter-new
      containers:
      - name: prometheus-adapter
        image: "k8s.gcr.io/prometheus-adapter/prometheus-adapter:v0.9.1"
        imagePullPolicy: IfNotPresent
        args:
        - /adapter
        - --secure-port=6443
        - --cert-dir=/tmp/cert
        - --logtostderr=true
        - --prometheus-url=http://release-name-prometheus-server-new:80
        - --prometheus-header=Authorization=Basic dXNlcjpzbmFwcHlmbG93
        - --metrics-relist-interval=1m
        - --v=4
        - --config=/etc/adapter/config.yaml
        ports:
        - containerPort: 6443
          name: https
        livenessProbe:
          httpGet:
            path: /healthz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: https
            scheme: HTTPS
          initialDelaySeconds: 30
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 200m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop: ["all"]
          readOnlyRootFilesystem: true
          runAsNonRoot: true
          runAsUser: 10001
        volumeMounts:
        - mountPath: /etc/adapter/
          name: config
          readOnly: true
        - mountPath: /tmp
          name: tmp
      nodeSelector:
        {}
      affinity:
        {}
      priorityClassName: 
      securityContext:
        fsGroup: 10001
      tolerations:
        []
      volumes:
      - name: config
        configMap:
          name: release-name-prometheus-adapter-new
      - name: tmp
        emptyDir: {}
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus/templates/server/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "server-new"
    app: prometheus
    release: release-name
    chart: prometheus-15.0.2
    heritage: Helm
  name: release-name-prometheus-server-new
  namespace: default
spec:
  selector:
    matchLabels:
      component: "server-new"
      app: prometheus
      release: release-name
  replicas: 1
  strategy:
    type: Recreate
    rollingUpdate: null
  template:
    metadata:
      labels:
        component: "server-new"
        app: prometheus
        release: release-name
        chart: prometheus-15.0.2
        heritage: Helm
    spec:
      enableServiceLinks: true
      serviceAccountName: default
      containers:
        - name: prometheus-server-new
          image: "quay.io/prometheus/prometheus:v2.31.1"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=3d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
            - --web.config.file=/etc/config/web.config.yml
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
              scheme: 
              httpHeaders:
              - name: Authorization
                value: Basic dXNlcjpzbmFwcHlmbG93
            initialDelaySeconds: 60
            periodSeconds: 60
            timeoutSeconds: 30
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
              scheme: 
              httpHeaders:
              - name: Authorization
                value: Basic dXNlcjpzbmFwcHlmbG93
            initialDelaySeconds: 60
            periodSeconds: 60
            timeoutSeconds: 30
            failureThreshold: 3
            successThreshold: 1
          resources:
            limits:
              cpu: 500m
              memory: 3Gi
            requests:
              cpu: 250m
              memory: 2Gi
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      hostNetwork: 
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: release-name-prometheus-server-new
        - name: storage-volume
          persistentVolumeClaim:
            claimName: release-name-prometheus-server-new
---
# Source: sf-datapath/charts/cp-kafka-rest/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-cp-kafka-rest
  labels:
    app: cp-kafka-rest
    chart: cp-kafka-rest-0.1.0
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cp-kafka-rest
      release: release-name
  template:
    metadata:
      labels:
        app: cp-kafka-rest
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
        snappyflow/component: kafka
      annotations:
        checksum/config: 9ee018f1257ef2334d3328c780c353d4ad15bce924c9ce3c541436c204e11c93
        checksum/jmx-config: 8ece2ce211b91c045a034f7eb8118a9d789b5964f21dd9018d2e670f6b281fc5
        prometheus.io/scrape: "true"
        prometheus.io/port: "5556"
    spec:
      priorityClassName: release-name-priority-class
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "cp-kafka-rest"
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
      containers:
        - name: exporter
          image: "solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143"
          imagePullPolicy: "IfNotPresent"
          command:
          - java
          - -XX:+UnlockExperimentalVMOptions
          - -XX:+UseCGroupMemoryLimitForHeap
          - -XX:MaxRAMFraction=1
          - -XshowSettings:vm
          - -jar
          - jmx_prometheus_httpserver.jar
          - "5556"
          - /etc/jmx-kafka-rest/jmx-kafka-rest-prometheus.yml
          ports:
          - containerPort: 5556
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          volumeMounts:
          - name: jmx-config
            mountPath: /etc/jmx-kafka-rest
          - name: memory-util-logs
            mountPath: /memory-util-logs
        - name: server
          image: "snappyflowml/sf-kafka-rest:6"
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: rest-proxy
              containerPort: 8082
              protocol: TCP
            - containerPort: 5555
              name: jmx
          startupProbe:
            tcpSocket:
              port: 8082
            initialDelaySeconds: 60
            periodSeconds: 5
            successThreshold: 1
            failureThreshold: 20
            timeoutSeconds: 4
          livenessProbe:
            httpGet:
              path: /
              port: 8082
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 30
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: 500m
              memory: 896Mi
            requests:
              cpu: 50m
              memory: 600Mi
          env:
          - name: LOGARCH_AUTHENTICATOR_URL
            value: "http://release-name-authenticator"
          - name: KAFKAREST_LOG4J_OPTS
            value: -Dlog4j.configuration=file:///etc/customlog4j/connect-log4j.properties
          - name: KAFKA_REST_HOST_NAME
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: KAFKA_REST_BOOTSTRAP_SERVERS
            value: localhost:9092,localhost1:9092
          - name: KAFKA_REST_SCHEMA_REGISTRY_URL
            value: http://release-name-cp-schema-registry:8081
          - name: KAFKAREST_HEAP_OPTS
            value: "-Xms210M -Xmx600M -XX:-EnableCPUMonitor -Xtune:virtualized -XX:+UseContainerSupport -XX:+CompactStrings -Xverbosegclog:/memory-util-logs/gc-logs/%seq.xml,5,20 -Xdump:system:defaults:file=/memory-util-logs/system-dumps/%seq.dmp -Xdump:heap:defaults:file=/memory-util-logs/heap-dumps/%seq.phd -Xdump:java:defaults:file=/memory-util-logs/java-dumps/%seq.txt -Xdump:system+heap+java"
          - name: "KAFKA_REST_KAFKA_REST_RESOURCE_EXTENSION_CLASS"
            value: "com.maplelabs.kafka.rest.auth.CustomAuthSecurityRestExtension"
          - name: "KAFKA_REST_LISTENERS"
            value: "https://0.0.0.0:8081,http://0.0.0.0:8082"
          - name: "KAFKA_REST_MAX_REQUEST_SIZE"
            value: "10485760"
          - name: "KAFKA_REST_PRODUCER_RETRIES"
            value: "5"
          - name: KAFKAREST_JMX_PORT
            value: "5555"
          volumeMounts:
          - name: connect-log4j-properties
            mountPath: /etc/customlog4j
          - name: memory-util-logs
            mountPath: /memory-util-logs
      imagePullSecrets:
        - name: xxxx
      volumes:
      - name: jmx-config
        configMap:
          name: release-name-cp-kafka-rest-jmx-configmap
      - name: connect-log4j-properties
        configMap:
          name: release-name-cp-kafka-rest-kafka-log
      - name: memory-util-logs
        emptyDir: {}
---
# Source: sf-datapath/charts/cp-schema-registry/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-cp-schema-registry
  labels:
    app: cp-schema-registry
    chart: cp-schema-registry-0.1.0
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cp-schema-registry
      release: release-name
  template:
    metadata:
      labels:
        app: cp-schema-registry
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
        snappyflow/component: kafka
      annotations:
        checksum/config: 5ad4aae5937029b17b6147efa6dc035e6fa88fb1ad175a1dcb93d7dbb6f118f1
        checksum/jmx-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        prometheus.io/scrape: "false"
    spec:
      priorityClassName: release-name-priority-class
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
      containers:
        - name: schema-generator
          image: "snappyflowml/schema-generator:13"
          imagePullPolicy: "IfNotPresent"
          resources:
            limits:
              cpu: 100m
              memory: 1Gi
            requests:
              cpu: 10m
              memory: 256Mi
          volumeMounts:
          - name: schema-generator-config
            mountPath: /opt
        - name: server
          image: "confluentinc/cp-schema-registry:6.0.1"
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: schema-registry
              containerPort: 8081
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: 8081
              scheme: HTTP 
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 500m
              memory: 512Mi
          env:
          - name: SCHEMA_REGISTRY_LOG4J_OPTS
            value: -Dlog4j.configuration=file:///etc/customlog4j/connect-log4j.properties
          - name: SCHEMA_REGISTRY_HOST_NAME
            valueFrom:
              fieldRef:
                fieldPath: status.podIP
          - name: SCHEMA_REGISTRY_LISTENERS
            value: http://0.0.0.0:8081
          - name: SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS
            value: localhost:9092,localhost1:9092
          - name: SCHEMA_REGISTRY_KAFKASTORE_GROUP_ID
            value: release-name
          - name: SCHEMA_REGISTRY_MASTER_ELIGIBILITY
            value: "true"
          - name: SCHEMA_REGISTRY_HEAP_OPTS
            value: "-Xms256M -Xmx512M -XX:+UseG1GC -XX:MaxGCPauseMillis=100 -XX:G1HeapRegionSize=1M -XX:MaxMetaspaceSize=100M -XX:MinMetaspaceFreeRatio=10 -XX:MaxMetaspaceFreeRatio=30 -XX:MinHeapFreeRatio=10 -XX:MaxHeapFreeRatio=30"
          
          - name: SCHEMA_REGISTRY_KAFKASTORE_INIT_TIMEOUT_MS
            value: "300000"
          
          - name: JMX_PORT
            value: "5555"
          volumeMounts:
          - name: connect-log4j-properties
            mountPath: /etc/customlog4j
      imagePullSecrets:
        - name: xxxx
      volumes:
      - name: schema-generator-config
        configMap:
          name: release-name-cp-schema-registry-schema-generator-configmap
      - name: connect-log4j-properties
        configMap:
          name: release-name-cp-schema-registry-kafka-log
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: release-name-nginx-ingress-controller
  namespace: default
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: nginx-ingress
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: controller
  replicas: 1
  revisionHistoryLimit: 10
  minReadySeconds: 0
  template:
    metadata:
      annotations:
        prometheus.io/port: "9113"
        prometheus.io/scrape: "true"
      labels:
        app.kubernetes.io/name: nginx-ingress
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: controller
    spec:
      dnsPolicy: ClusterFirst
      containers:
        - name: controller
          image: "registry.k8s.io/ingress-nginx/controller:v1.5.1@sha256:4ba73c697770664c1e00e9f968de14e08f606ff961c76e5d7033a4a9c593c629"
          imagePullPolicy: IfNotPresent
          lifecycle: 
            preStop:
              exec:
                command:
                - /wait-shutdown
          args:
            - /nginx-ingress-controller
            - --publish-service=$(POD_NAMESPACE)/release-name-nginx-ingress-controller
            - --election-id=release-name-nginx-ingress-leader
            - --controller-class=k8s.io/ingress-nginx
            - --ingress-class=apm-saas-datapath
            - --configmap=$(POD_NAMESPACE)/release-name-nginx-ingress-controller
            - --validating-webhook=:8443
            - --validating-webhook-certificate=/usr/local/certificates/cert
            - --validating-webhook-key=/usr/local/certificates/key
          securityContext: 
            capabilities:
              drop:
              - ALL
              add:
              - NET_BIND_SERVICE
            runAsUser: 101
            allowPrivilegeEscalation: true
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: LD_PRELOAD
              value: /usr/local/lib/libmimalloc.so
          livenessProbe: 
            failureThreshold: 5
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          readinessProbe: 
            failureThreshold: 3
            httpGet:
              path: /healthz
              port: 10254
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
            - name: webhook
              containerPort: 8443
              protocol: TCP
          volumeMounts:
            - name: webhook-cert
              mountPath: /usr/local/certificates/
              readOnly: true
          resources: 
            requests:
              cpu: 100m
              memory: 90Mi
      nodeSelector: 
        kubernetes.io/os: linux
      serviceAccountName: release-name-nginx-ingress
      terminationGracePeriodSeconds: 300
      volumes:
        - name: webhook-cert
          secret:
            secretName: release-name-nginx-ingress-admission
---
# Source: sf-datapath/charts/sfk-interface/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-sfk-interface
  labels:
    helm.sh/chart: sfk-interface-0.1.0
    release: release-name
    app.kubernetes.io/name: sfk-interface
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: sfk-interface
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        checksum/config: b9eece89d8c650736c0c73ace3eb8f6fd1fc417e6f5fc43a47761f621359d1b3
      labels:
        app.kubernetes.io/name: sfk-interface
        app.kubernetes.io/instance: release-name
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
    spec:
      priorityClassName: release-name-priority-class
      imagePullSecrets:
        - name: xxxx
      serviceAccountName: default
      securityContext:
        {}
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'release-name-postgresql':5432/'archival'; do sleep 3; done"
      containers:
        - name: "sfk-interface"
          securityContext:
            {}
          image: "snappyflowml/sfk-interface:26"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: release-name-sfk-interface-config
              mountPath: "/etc/conf"
              readOnly: true
          resources:
            limits:
              cpu: 100m
              memory: 100Mi
            requests:
              cpu: 10m
              memory: 50Mi
      volumes:
        - name: release-name-sfk-interface-config
          configMap:
            name: release-name-sfk-interface-config
---
# Source: sf-datapath/charts/signatures-and-kafka-apis/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-signatures-and-kafka-apis
  namespace: default
  labels:
    app.kubernetes.io/name: signatures-and-kafka-apis
    helm.sh/chart: signatures-and-kafka-apis-0.1.0
    app.kubernetes.io/instance: release-name
    release: release-name
    app.kubernetes.io/version: "1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: signatures-and-kafka-apis
      app.kubernetes.io/instance: release-name
  template:
    metadata:
      annotations:
        checksum/config: d5f254205a1184777970c9de74ff752f1341c04631111618baa42f1d14a8f89c
      labels:
        app.kubernetes.io/name: signatures-and-kafka-apis
        app.kubernetes.io/instance: release-name
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
    spec:
      priorityClassName: release-name-priority-class
      imagePullSecrets:
        - name: xxxx
      initContainers:
        - name: db-ready
          image: "bitnami/postgresql:11.5.0-debian-9-r34"
          imagePullPolicy: IfNotPresent
          command:
            - sh
            - -c
            - "until pg_isready -d postgresql://'archive':'archive123'@'release-name-postgresql':5432/'archival'; do sleep 3; done"
      containers:
        - name: signatures-and-kafka-apis
          image: "snappyflowml/signatures:10"
          imagePullPolicy: IfNotPresent
          env:
          - name: archival
            value: "true"
          - name: BOOTSTRAP_SERVERS
            value: "localhost:9092,localhost1:9092"
          # TODO: This should be dynamically detected in order to apply quotas
          - name: NUM_BOOTSTRAP_SERVERS
            value: "3"
          - name: QUOTA_BUFFER_PCNT
            value: "5"
          volumeMounts:
          - name: config-volume
            mountPath: /etc/conf
          readinessProbe:
            httpGet:
              path: "/signatures/check"
              port: 8888
            timeoutSeconds: 10
            initialDelaySeconds: 180
            periodSeconds: 30
            failureThreshold: 3
          resources:
            limits:
              cpu: 200m
              memory: 512Mi
            requests:
              cpu: 50m
              memory: 256Mi
      volumes:
        - name: config-volume
          configMap:
            name: release-name-signatures-and-kafka-apis
---
# Source: sf-datapath/charts/autoscaling/templates/archival-kafka-connect-hpa.yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: release-name-archival-kafka-connect
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: release-name-archival-kafka-connect
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageValue: "850m"
  - type: External
    external:
      metricName: kafka_bytesinpersec_oneminuterate_for_log_and_metric
      targetValue: 2315000
---
# Source: sf-datapath/charts/autoscaling/templates/cp-kafka-rest-hpa.yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: release-name-cp-kafka-rest
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: release-name-cp-kafka-rest
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Pods
    pods:
      metricName: kafka_rest_heap_utilization_pcnt
      targetAverageValue: "85"
  - type: Resource
    resource:
      name: cpu
      targetAverageValue: "450m"
---
# Source: sf-datapath/charts/autoscaling/templates/es-kafka-connect-hpa.yaml
apiVersion: autoscaling/v2beta1
kind: HorizontalPodAutoscaler
metadata:
  name: release-name-es-kafka-connect
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: StatefulSet
    name: release-name-es-kafka-connect
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      targetAverageValue: "850m"
  - type: External
    external:
      metricName: kafka_bytesinpersec_oneminuterate
      targetValue: 2315000
---
# Source: sf-datapath/charts/archival-kafka-connect/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-archival-kafka-connect
  labels:
    app: archival-kafka-connect
    chart: archival-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: archival-kafka-connect
      release: release-name
  serviceName: release-name-archival-kafka-connect
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: archival-kafka-connect
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
        snappyflow/component: kafka
      annotations:
        checksum/config: 51e2a920ac8ca5ae303e99b30857b11e37a4bcaa2b928fb24ae5842fff0c668a
        checksum/jmx-config: 8e80eb9a9378ddae5a86d2b38a54a78669a7c61afb88f8740da2b9f400af4d0e
        prometheus.io/scrape: "true"
        prometheus.io/port: "5556"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "es-kafka-connect"
          - weight: 2
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "archival-kafka-connect"
      priorityClassName: release-name-priority-class
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
      containers:
        - name: exporter
          image: "solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143"
          imagePullPolicy: "IfNotPresent"
          command:
          - java
          - -XX:+UnlockExperimentalVMOptions
          - -XX:+UseCGroupMemoryLimitForHeap
          - -XX:MaxRAMFraction=1
          - -XshowSettings:vm
          - -jar
          - jmx_prometheus_httpserver.jar
          - "5556"
          - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml
          ports:
          - containerPort: 5556
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          volumeMounts:
          - name: jmx-config
            mountPath: /etc/jmx-kafka-connect
          - name: memory-util-logs
            mountPath: /memory-util-logs
        - name: connector
          image: "snappyflowml/arch-kafka-connect:17"
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: arch-connect
              containerPort: 8083
              protocol: TCP
            - containerPort: 5555
              name: jmx
          startupProbe:
            tcpSocket:
              port: 8083
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 20
            timeoutSeconds: 9
          livenessProbe:
            httpGet:
              path: /connectors
              port: 8083
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: "1"
              memory: 3Gi
            requests:
              cpu: "1"
              memory: 1536Mi
          env:
            - name: CONNECT_REST_ADVERTISED_HOST_NAME
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: KAFKA_LOG4J_OPTS
              value: -Dlog4j.configuration=file:///etc/customlog4j/connect-log4j.properties
            - name: CONNECT_BOOTSTRAP_SERVERS
              value: localhost:9092,localhost1:9092
            - name: CONNECT_GROUP_ID
              value: default-release-name-s3-connect
            - name: CONNECT_CONFIG_STORAGE_TOPIC
              value: default-release-name-s3-connect-config
            - name: CONNECT_OFFSET_STORAGE_TOPIC
              value: default-release-name-s3-connect-offset
            - name: CONNECT_STATUS_STORAGE_TOPIC
              value: default-release-name-s3-connect-status
            - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL
              value: http://release-name-cp-schema-registry:8081
            - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL
              value: http://release-name-cp-schema-registry:8081
            - name: KAFKA_HEAP_OPTS
              value: "-Xms250M -Xmx1300M -Xmns200M -Xmnx800M -XX:MaxDirectMemorySize=100M -Xcodecachetotal50M -Xmso128K -Xgc:concurrentScavenge -XX:-EnableCPUMonitor -Xtune:virtualized -XX:+CompactStrings -Xverbosegclog:/memory-util-logs/gc-logs/%seq.xml,20,100 -Xdump:system:defaults:file=/memory-util-logs/system-dumps/%seq.dmp -Xdump:heap:defaults:file=/memory-util-logs/heap-dumps/%seq.phd -Xdump:java:defaults:file=/memory-util-logs/java-dumps/%seq.txt -Xdump:system+heap+java"
            - name: "CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR"
              value: "3"
            - name: "CONNECT_CONNECT_PROTOCOL"
              value: "apm_sessioned"
            - name: "CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY"
              value: "All"
            - name: "CONNECT_CONSUMER_FETCH_MAX_BYTES"
              value: "1048576"
            - name: "CONNECT_CONSUMER_MAX_PARTITION_FETCH_BYTES"
              value: "1048576"
            - name: "CONNECT_CONSUMER_MAX_POLL_RECORDS"
              value: "5000"
            - name: "CONNECT_CONSUMER_PARTITION_ASSIGNMENT_STRATEGY"
              value: "org.apache.kafka.clients.consumer.RoundRobinAssignor"
            - name: "CONNECT_INTERNAL_KEY_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_INTERNAL_VALUE_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_KEY_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE"
              value: "false"
            - name: "CONNECT_OFFSET_FLUSH_INTERVAL_MS"
              value: "60000"
            - name: "CONNECT_OFFSET_FLUSH_TIMEOUT_MS"
              value: "45000"
            - name: "CONNECT_OFFSET_STORAGE_PARTITIONS"
              value: "3"
            - name: "CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR"
              value: "3"
            - name: "CONNECT_PLUGIN_PATH"
              value: "/usr/share/java,/etc/kafka-connect/custom_smt,/usr/share/confluent-hub-components"
            - name: "CONNECT_SCHEDULED_REBALANCE_MAX_DELAY_MS"
              value: "60000"
            - name: "CONNECT_STATUS_STORAGE_PARTITIONS"
              value: "3"
            - name: "CONNECT_STATUS_STORAGE_REPLICATION_FACTOR"
              value: "3"
            - name: "CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS"
              value: "180000"
            - name: "CONNECT_VALUE_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE"
              value: "false"
            - name: KAFKA_JMX_PORT
              value: "5555"
          volumeMounts:
          - name: connect-log4j-properties
            mountPath: /etc/customlog4j
          - name: memory-util-logs
            mountPath: /memory-util-logs
      imagePullSecrets:
        - name: xxxx
      volumes:
      - name: jmx-config
        configMap:
          name: release-name-archival-kafka-connect-jmx-configmap
      - name: connect-log4j-properties
        configMap:
          name: release-name-archival-kafka-connect-kafka-log
      - name: memory-util-logs
        emptyDir: {}
---
# Source: sf-datapath/charts/es-kafka-connect/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: release-name-es-kafka-connect
  labels:
    app: es-kafka-connect
    chart: es-kafka-connect-0.1.0
    release: release-name
    heritage: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: es-kafka-connect
      release: release-name
  serviceName: release-name-es-kafka-connect
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: es-kafka-connect
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
        snappyflow/component: kafka
      annotations:
        checksum/config: d063ace7f6bb7787427e2a1796e636f567491596c3fe26354d011693186ce7c3
        checksum/jmx-config: d98dcb1ef405d4e67183265bcfd6b626f2b025cce0d41d81de7ba502d299a908
        prometheus.io/scrape: "true"
        prometheus.io/port: "5556"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "archival-kafka-connect"
          - weight: 2
            podAffinityTerm:
              topologyKey: "kubernetes.io/hostname"
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - "es-kafka-connect"
      priorityClassName: release-name-priority-class
      initContainers:
        - name: kafka-ready
          image: "snappyflowml/kafka-zk-check:alpha"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - ./kafka-init.sh
            - localhost:9092,localhost1:9092
      containers:
        - name: exporter
          image: "solsson/kafka-prometheus-jmx-exporter@sha256:6f82e2b0464f50da8104acd7363fb9b995001ddff77d248379f8788e78946143"
          imagePullPolicy: "IfNotPresent"
          command:
          - java
          - -XX:+UnlockExperimentalVMOptions
          - -XX:+UseCGroupMemoryLimitForHeap
          - -XX:MaxRAMFraction=1
          - -XshowSettings:vm
          - -jar
          - jmx_prometheus_httpserver.jar
          - "5556"
          - /etc/jmx-kafka-connect/jmx-kafka-connect-prometheus.yml
          ports:
          - containerPort: 5556
          resources:
            limits:
              cpu: 500m
              memory: 1Gi
            requests:
              cpu: 100m
              memory: 256Mi
          volumeMounts:
          - name: jmx-config
            mountPath: /etc/jmx-kafka-connect
          - name: memory-util-logs
            mountPath: /memory-util-logs
        - name: connector
          image: "snappyflowml/apm-kafka-connect:18"
          imagePullPolicy: "IfNotPresent"
          ports:
            - name: es-connect
              containerPort: 8083
              protocol: TCP
            - containerPort: 5555
              name: jmx
          startupProbe:
            tcpSocket:
              port: 8083
            initialDelaySeconds: 120
            periodSeconds: 10
            successThreshold: 1
            failureThreshold: 20
            timeoutSeconds: 9
          livenessProbe:
            httpGet:
              path: /connectors
              port: 8083
              scheme: HTTP
            initialDelaySeconds: 300
            periodSeconds: 60
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          resources:
            limits:
              cpu: "1"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 2560Mi
          env:
            - name: CONNECT_REST_ADVERTISED_HOST_NAME
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: KAFKA_LOG4J_OPTS
              value: -Dlog4j.configuration=file:///etc/customlog4j/connect-log4j.properties
            - name: CONNECT_BOOTSTRAP_SERVERS
              value: localhost:9093,localhost1:9093
            - name: CONNECT_GROUP_ID
              value: default-release-name-es-connect
            - name: CONNECT_CONFIG_STORAGE_TOPIC
              value: default-release-name-es-connect-config
            - name: CONNECT_OFFSET_STORAGE_TOPIC
              value: default-release-name-es-connect-offset
            - name: CONNECT_STATUS_STORAGE_TOPIC
              value: default-release-name-es-connect-status
            - name: CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL
              value: http://release-name-cp-schema-registry:8081
            - name: CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL
              value: http://release-name-cp-schema-registry:8081
            - name: KAFKA_HEAP_OPTS
              value: "-Xms450M -Xmx1800M -Xmns350M -Xmnx1400M -XX:MaxDirectMemorySize=200M -Xcodecachetotal50M -Xmso128K -Xgc:concurrentScavenge -XX:-EnableCPUMonitor -Xtune:virtualized -XX:+CompactStrings -Xverbosegclog:/memory-util-logs/gc-logs/%seq.xml,20,100 -Xdump:system:defaults:file=/memory-util-logs/system-dumps/%seq.dmp -Xdump:heap:defaults:file=/memory-util-logs/heap-dumps/%seq.phd -Xdump:java:defaults:file=/memory-util-logs/java-dumps/%seq.txt -Xdump:system+heap+java"
            - name: "CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR"
              value: "3"
            - name: "CONNECT_CONNECT_PROTOCOL"
              value: "apm_sessioned"
            - name: "CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY"
              value: "All"
            - name: "CONNECT_CONSUMER_FETCH_MAX_BYTES"
              value: "1048576"
            - name: "CONNECT_CONSUMER_MAX_PARTITION_FETCH_BYTES"
              value: "1048576"
            - name: "CONNECT_CONSUMER_MAX_POLL_RECORDS"
              value: "5000"
            - name: "CONNECT_CONSUMER_PARTITION_ASSIGNMENT_STRATEGY"
              value: "org.apache.kafka.clients.consumer.RoundRobinAssignor"
            - name: "CONNECT_CONSUMER_SASL_JAAS_CONFIG"
              value: "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"snp-kconnect\" password=\"snp-kconnect\";"
            - name: "CONNECT_CONSUMER_SASL_MECHANISM"
              value: "PLAIN"
            - name: "CONNECT_CONSUMER_SECURITY_PROTOCOL"
              value: "SASL_PLAINTEXT"
            - name: "CONNECT_INTERNAL_KEY_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_INTERNAL_VALUE_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_KEY_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE"
              value: "false"
            - name: "CONNECT_OFFSET_FLUSH_INTERVAL_MS"
              value: "60000"
            - name: "CONNECT_OFFSET_FLUSH_TIMEOUT_MS"
              value: "45000"
            - name: "CONNECT_OFFSET_STORAGE_PARTITIONS"
              value: "3"
            - name: "CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR"
              value: "3"
            - name: "CONNECT_PLUGIN_PATH"
              value: "/usr/share/java,/etc/kafka-connect/custom_smt,/usr/share/confluent-hub-components"
            - name: "CONNECT_SASL_JAAS_CONFIG"
              value: "org.apache.kafka.common.security.plain.PlainLoginModule required username=\"snp-kconnect\" password=\"snp-kconnect\";"
            - name: "CONNECT_SASL_MECHANISM"
              value: "PLAIN"
            - name: "CONNECT_SCHEDULED_REBALANCE_MAX_DELAY_MS"
              value: "60000"
            - name: "CONNECT_SECURITY_PROTOCOL"
              value: "SASL_PLAINTEXT"
            - name: "CONNECT_STATUS_STORAGE_PARTITIONS"
              value: "3"
            - name: "CONNECT_STATUS_STORAGE_REPLICATION_FACTOR"
              value: "3"
            - name: "CONNECT_TASK_SHUTDOWN_GRACEFUL_TIMEOUT_MS"
              value: "180000"
            - name: "CONNECT_VALUE_CONVERTER"
              value: "org.apache.kafka.connect.json.JsonConverter"
            - name: "CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE"
              value: "false"
            - name: KAFKA_JMX_PORT
              value: "5555"
          volumeMounts:
          - name: connect-log4j-properties
            mountPath: /etc/customlog4j
          - name: memory-util-logs
            mountPath: /memory-util-logs
      imagePullSecrets:
        - name: xxxx
      volumes:
      - name: jmx-config
        configMap:
          name: release-name-es-kafka-connect-jmx-configmap
      - name: connect-log4j-properties
        configMap:
          name: release-name-es-kafka-connect-kafka-log
      - name: memory-util-logs
        emptyDir: {}
---
# Source: sf-datapath/charts/sfk-interface/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: release-name-quota-util-stats
  namespace: default
  labels:
    helm.sh/chart: sfk-interface-0.1.0
    release: release-name
    app.kubernetes.io/name: sfk-interface
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/30 * * * *"
  successfulJobsHistoryLimit: 0
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: release-name
        spec:
          containers:
          - name: main
            image: snappyflowml/sfk-quota-stats:5
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - name: release-name-sfk-interface-config
                mountPath: "/etc/conf"
          restartPolicy: Never
          volumes:
            - name: release-name-sfk-interface-config
              configMap:
                name: release-name-sfk-interface-config
---
# Source: sf-datapath/charts/sfk-interface/templates/cron.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: release-name-sinks-monitor
  namespace: default
  labels:
    helm.sh/chart: sfk-interface-0.1.0
    release: release-name
    app.kubernetes.io/name: sfk-interface
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  concurrencyPolicy: Forbid
  schedule: "*/15 * * * *"
  successfulJobsHistoryLimit: 1
  failedJobsHistoryLimit: 1
  startingDeadlineSeconds: 200
  jobTemplate:
    spec:
      backoffLimit: 0
      template:
        metadata:
          labels:
            release: release-name
        spec:
          containers:
          - name: main
            image: snappyflowml/sink-monitor-consumer-groups:5
            imagePullPolicy: IfNotPresent
            command: ["bash"]
            args: ["-c", "python app.py connect-es"]
            volumeMounts:
              - name: release-name-sfk-interface-config
                mountPath: "/etc/config"
          restartPolicy: Never
          volumes:
            - name: release-name-sfk-interface-config
              configMap:
                name: release-name-sfk-interface-config
---
# Source: sf-datapath/charts/nginx-ingress/templates/controller-ingressclass.yaml
# We don't support namespaced ingressClass yet
# So a ClusterRole and a ClusterRoleBinding is required
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: controller
  name: apm-saas-datapath
spec:
  controller: k8s.io/ingress-nginx
---
# Source: sf-datapath/charts/sfk-interface/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-sfk-interface
  namespace: default
  annotations:
    kubernetes.io/ingress.class: apm-saas-datapath
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: release-name-usercreds
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - ok'
    nginx.ingress.kubernetes.io/auth-secret-type: "auth-file"
spec:
  rules:
  - http:
      paths:
      - path: /sfkinterface
        backend:
          service:
            name: release-name-sfk-interface
            port: 
              number: 80
        pathType: Prefix
      - path: /profile-quotas
        backend:
          service:
            name: release-name-sfk-interface
            port: 
              number: 80
        pathType: Prefix
---
# Source: sf-datapath/charts/signatures-and-kafka-apis/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: release-name-signatures-and-kafka-apis
  namespace: default
  annotations:
    kubernetes.io/ingress.class: apm-saas-datapath
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/auth-type: basic
    nginx.ingress.kubernetes.io/auth-secret: release-name-usercreds
    nginx.ingress.kubernetes.io/auth-realm: 'Authentication Required - ok'
    nginx.ingress.kubernetes.io/auth-secret-type: "auth-file"
spec:
  rules:
  - host: 
    http:
      paths:
      - path: /signatures
        backend:
          service:
            name: release-name-signatures-and-kafka-apis
            port: 
              number: 80
        pathType: Prefix
      - path: /kafka-info
        backend:
          service:
            name: release-name-signatures-and-kafka-apis
            port: 
              number: 80
        pathType: Prefix
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/custom-metrics-apiservice.yaml
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: v1beta1.custom.metrics.k8s.io
spec:
  service:
    name: release-name-prometheus-adapter-new
    namespace: "default"
  group: custom.metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100
---
# Source: sf-datapath/charts/autoscaling/charts/prometheus-adapter/templates/external-metrics-apiservice.yaml
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  labels:    
    helm.sh/chart: prometheus-adapter-3.0.1
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: prometheus-adapter-new
    app.kubernetes.io/name: prometheus-adapter-new
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "v0.9.1"
  name: v1beta1.external.metrics.k8s.io
spec:
  service:
    name: release-name-prometheus-adapter-new
    namespace: "default"
  group: external.metrics.k8s.io
  version: v1beta1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 100
  versionPriority: 100
---
# Source: sf-datapath/templates/priority-class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: release-name-priority-class
value: 100
preemptionPolicy: Never
globalDefault: false
description: "This priority class is used by sf-datapath components: es/s3 connect, kafka rest/registry servers"
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/validating-webhook.yaml
# before changing this value, check the required kubernetes version
# https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#prerequisites
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  annotations:
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
  name: release-name-nginx-ingress-admission
webhooks:
  - name: validate.nginx.ingress.kubernetes.io
    matchPolicy: Equivalent
    rules:
      - apiGroups:
          - networking.k8s.io
        apiVersions:
          - v1
        operations:
          - CREATE
          - UPDATE
        resources:
          - ingresses
    failurePolicy: Fail
    sideEffects: None
    admissionReviewVersions:
      - v1
    clientConfig:
      service:
        namespace: "default"
        name: release-name-nginx-ingress-controller-admission
        path: /networking/v1/ingresses
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/job-patch/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: release-name-nginx-ingress-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/job-patch/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: release-name-nginx-ingress-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
rules:
  - apiGroups:
      - admissionregistration.k8s.io
    resources:
      - validatingwebhookconfigurations
    verbs:
      - get
      - update
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/job-patch/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name:  release-name-nginx-ingress-admission
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: release-name-nginx-ingress-admission
subjects:
  - kind: ServiceAccount
    name: release-name-nginx-ingress-admission
    namespace: "default"
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/job-patch/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name:  release-name-nginx-ingress-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - create
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/job-patch/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: release-name-nginx-ingress-admission
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade,post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-nginx-ingress-admission
subjects:
  - kind: ServiceAccount
    name: release-name-nginx-ingress-admission
    namespace: "default"
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/job-patch/job-createSecret.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-nginx-ingress-admission-create
  namespace: default
  annotations:
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
spec:
  template:
    metadata:
      name: release-name-nginx-ingress-admission-create
      labels:
        helm.sh/chart: nginx-ingress-4.4.0
        app.kubernetes.io/name: nginx-ingress
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "1.5.1"
        app.kubernetes.io/part-of: nginx-ingress
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: admission-webhook
    spec:
      containers:
        - name: create
          image: "registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f"
          imagePullPolicy: IfNotPresent
          args:
            - create
            - --host=release-name-nginx-ingress-controller-admission,release-name-nginx-ingress-controller-admission.$(POD_NAMESPACE).svc
            - --namespace=$(POD_NAMESPACE)
            - --secret-name=release-name-nginx-ingress-admission
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          securityContext: 
            allowPrivilegeEscalation: false
      restartPolicy: OnFailure
      serviceAccountName: release-name-nginx-ingress-admission
      nodeSelector: 
        kubernetes.io/os: linux
      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: sf-datapath/charts/nginx-ingress/templates/admission-webhooks/job-patch/job-patchWebhook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-nginx-ingress-admission-patch
  namespace: default
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
  labels:
    helm.sh/chart: nginx-ingress-4.4.0
    app.kubernetes.io/name: nginx-ingress
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "1.5.1"
    app.kubernetes.io/part-of: nginx-ingress
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: admission-webhook
spec:
  template:
    metadata:
      name: release-name-nginx-ingress-admission-patch
      labels:
        helm.sh/chart: nginx-ingress-4.4.0
        app.kubernetes.io/name: nginx-ingress
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "1.5.1"
        app.kubernetes.io/part-of: nginx-ingress
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: admission-webhook
    spec:
      containers:
        - name: patch
          image: "registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20220916-gd32f8c343@sha256:39c5b2e3310dc4264d638ad28d9d1d96c4cbb2b2dcfb52368fe4e3c63f61e10f"
          imagePullPolicy: IfNotPresent
          args:
            - patch
            - --webhook-name=release-name-nginx-ingress-admission
            - --namespace=$(POD_NAMESPACE)
            - --patch-mutating=false
            - --secret-name=release-name-nginx-ingress-admission
            - --patch-failure-policy=Fail
          env:
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          securityContext: 
            allowPrivilegeEscalation: false
      restartPolicy: OnFailure
      serviceAccountName: release-name-nginx-ingress-admission
      nodeSelector: 
        kubernetes.io/os: linux
      securityContext:
        fsGroup: 2000
        runAsNonRoot: true
        runAsUser: 2000
---
# Source: sf-datapath/templates/system-migration.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: release-name-system-migration
  namespace: default
  labels:
    app.kubernetes.io/managed-by: "Helm"
    app.kubernetes.io/instance: "release-name"
    helm.sh/chart: "sf-datapath-2.0.62"
  annotations:
    "helm.sh/hook": post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation
spec:
  template:
    metadata:
      name: release-name-system-migraton
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/instance: "release-name"
        helm.sh/chart: "sf-datapath-2.0.62"
        release: release-name
        snappyflow/projectname: snappyflow-app
        snappyflow/appname: sf-datapath
    spec:
      restartPolicy: Never
      containers:
      - name: main
        image: snappyflowml/datapath-system-migration:8
        imagePullPolicy: IfNotPresent
        volumeMounts:
          - name: release-name-system-migration-config
            mountPath: "/etc/conf"
            readOnly: true
      volumes:
        - name: release-name-system-migration-config
          configMap:
            name: release-name-system-migration-config
